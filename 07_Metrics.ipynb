{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Model for Inference at Production Scale\n",
    "\n",
    "## 07 - Metrics\n",
    "-------\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [Prometheus](#prometheus)\n",
    "* [Exercise](#exercise)\n",
    "* [Docker Compose](#docker-compose)\n",
    "* [Next Steps](#next-steps)\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will look at some of the server metrics Triton has automatically collected for us. Triton provides Prometheus metrics indicating GPU and request statistics. By default, these metrics are available at http://localhost:8002/metrics. The metrics are only available by accessing the endpoint, and are not pushed or published to any remote server. The metric format is plain text so they can be viewed directly, for example by running the `curl` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HELP nv_inference_request_success Number of successful inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_success counter\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 30000.000000\n",
      "nv_inference_request_success{model=\"simple-pytorch-model-cpu\",version=\"1\"} 1000.000000\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 1.000000\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 10301.000000\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 1.000000\n",
      "nv_inference_request_success{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 1001.000000\n",
      "# HELP nv_inference_request_failure Number of failed inference requests, all batch sizes\n",
      "# TYPE nv_inference_request_failure counter\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 0.000000\n",
      "nv_inference_request_failure{model=\"simple-pytorch-model-cpu\",version=\"1\"} 0.000000\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 0.000000\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 0.000000\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 0.000000\n",
      "nv_inference_request_failure{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 0.000000\n",
      "# HELP nv_inference_count Number of inferences performed\n",
      "# TYPE nv_inference_count counter\n",
      "nv_inference_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 30000.000000\n",
      "nv_inference_count{model=\"simple-pytorch-model-cpu\",version=\"1\"} 1000.000000\n",
      "nv_inference_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 1.000000\n",
      "nv_inference_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 19601.000000\n",
      "nv_inference_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 1.000000\n",
      "nv_inference_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 1001.000000\n",
      "# HELP nv_inference_exec_count Number of model executions performed\n",
      "# TYPE nv_inference_exec_count counter\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 20833.000000\n",
      "nv_inference_exec_count{model=\"simple-pytorch-model-cpu\",version=\"1\"} 1000.000000\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 1.000000\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 10301.000000\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 1.000000\n",
      "nv_inference_exec_count{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 1001.000000\n",
      "# HELP nv_inference_request_duration_us Cummulative inference request duration in microseconds\n",
      "# TYPE nv_inference_request_duration_us counter\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 505444823.000000\n",
      "nv_inference_request_duration_us{model=\"simple-pytorch-model-cpu\",version=\"1\"} 353308071.000000\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 12673.000000\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 29364515.000000\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 172074.000000\n",
      "nv_inference_request_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 8809411.000000\n",
      "# HELP nv_inference_queue_duration_us Cummulative inference queuing duration in microseconds\n",
      "# TYPE nv_inference_queue_duration_us counter\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 284224451.000000\n",
      "nv_inference_queue_duration_us{model=\"simple-pytorch-model-cpu\",version=\"1\"} 376016.000000\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 36.000000\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 173910.000000\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 72.000000\n",
      "nv_inference_queue_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 20834.000000\n",
      "# HELP nv_inference_compute_input_duration_us Cummulative compute input duration in microseconds\n",
      "# TYPE nv_inference_compute_input_duration_us counter\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 95691045.000000\n",
      "nv_inference_compute_input_duration_us{model=\"simple-pytorch-model-cpu\",version=\"1\"} 139891.000000\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 2378.000000\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 5923041.000000\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 128.000000\n",
      "nv_inference_compute_input_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 233502.000000\n",
      "# HELP nv_inference_compute_infer_duration_us Cummulative compute inference duration in microseconds\n",
      "# TYPE nv_inference_compute_infer_duration_us counter\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 124986880.000000\n",
      "nv_inference_compute_infer_duration_us{model=\"simple-pytorch-model-cpu\",version=\"1\"} 349777020.000000\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 10249.000000\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 23195458.000000\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 171435.000000\n",
      "nv_inference_compute_infer_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 7902557.000000\n",
      "# HELP nv_inference_compute_output_duration_us Cummulative inference compute output duration in microseconds\n",
      "# TYPE nv_inference_compute_output_duration_us counter\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"dynamic-batching-model\",version=\"1\"} 436047.000000\n",
      "nv_inference_compute_output_duration_us{model=\"simple-pytorch-model-cpu\",version=\"1\"} 31630.000000\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp32-model\",version=\"1\"} 8.000000\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-tensorrt-fp16-model\",version=\"1\"} 51587.000000\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-onnx-model\",version=\"1\"} 22.000000\n",
      "nv_inference_compute_output_duration_us{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\",model=\"simple-pytorch-model\",version=\"1\"} 628516.000000\n",
      "# HELP nv_gpu_utilization GPU utilization rate [0.0 - 1.0)\n",
      "# TYPE nv_gpu_utilization gauge\n",
      "nv_gpu_utilization{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\"} 0.000000\n",
      "# HELP nv_gpu_memory_total_bytes GPU total memory, in bytes\n",
      "# TYPE nv_gpu_memory_total_bytes gauge\n",
      "nv_gpu_memory_total_bytes{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\"} 15843721216.000000\n",
      "# HELP nv_gpu_memory_used_bytes GPU used memory, in bytes\n",
      "# TYPE nv_gpu_memory_used_bytes gauge\n",
      "nv_gpu_memory_used_bytes{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\"} 10846797824.000000\n",
      "# HELP nv_gpu_power_usage GPU power usage in watts\n",
      "# TYPE nv_gpu_power_usage gauge\n",
      "nv_gpu_power_usage{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\"} 25.579000\n",
      "# HELP nv_gpu_power_limit GPU power management limit in watts\n",
      "# TYPE nv_gpu_power_limit gauge\n",
      "nv_gpu_power_limit{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\"} 70.000000\n",
      "# HELP nv_energy_consumption GPU energy consumption in joules since the Triton Server started\n",
      "# TYPE nv_energy_consumption counter\n",
      "nv_energy_consumption{gpu_uuid=\"GPU-45dcd2ba-97ce-3611-2e50-a5b7da799e59\"} 174987.650000\n"
     ]
    }
   ],
   "source": [
    "!curl triton:8002/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prometheus\"></a>\n",
    "### Prometheus\n",
    "\n",
    "For a format that's more pleasing on the eyes, Triton metrics are compatible with [Prometheus](https://prometheus.io/). Copy and paste your JupyterLab URL into the code cell below to generate a link to the Prometheus window. There will be a screen that looks like this:\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/Prom.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ec2-54-209-80-90.compute-1.amazonaws.com:9090/graph'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "my_url = \"http://ec2-54-209-80-90.compute-1.amazonaws.com/lab/lab/workspaces/auto-7/tree/07_Metrics.ipynb\"\n",
    "prometheus_url = my_url.rsplit(\".com\", 1)[0] + \".com:9090/graph\"\n",
    "prometheus_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we're pushing our hardware too much by looking at the GPU utilization. This could help us diagnose potential crashes or we could set up an [alert](https://prometheus.io/docs/alerting/latest/overview/) if we're close to 100% and need more resources.\n",
    "\n",
    "Click the earth icon, and select `nv_gpu_utilization`. Then, click \"Execute\".\n",
    "\n",
    "<div align=\"center\"><img src=\"./assets/Prom_Add_Metric.png\"></div>\n",
    "\n",
    "This will produce a line that looks like this:\n",
    "\n",
    "*nv_gpu_utilization{gpu_uuid=\"GPU-76eed5e4-a509-ea60-8ce3-5c9b82f9252b\", instance=\"triton:8002\", job=\"prometheus\"}*\n",
    "\n",
    "Click the `Graph` tab, and a graph will appear with the GPU utilization over the past hour.\n",
    "\n",
    "<a id=\"exercise\"></a>\n",
    "### Exercise #3 - Make your own dashboard\n",
    "\n",
    "Other metrics can be compared side by side. Scroll down to the bottom of the Prometheus window and click the \"Add Panel\" button on the left. This allows us to repeat the process above. Add a few more metrics of your choosing and analyze your previous activity. Can you recognize when you sent requests to Triton from the previous notebooks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"docker-compose\"></a>\n",
    "### Docker Compose\n",
    "\n",
    "One of the most straightforward ways of setting up both Triton and Prometheus is with a tool called [Docker Compose](https://docs.docker.com/compose/). It allows us to deploy multiple [Docker containers](https://www.docker.com/resources/what-container) that can share data and other resources. We highly recommend learning [Docker basics](https://www.docker.com/101-tutorial) before proceeding.\n",
    "\n",
    "For now, let's focus on `triton` and break down each of the keys:\n",
    "* [command](https://docs.docker.com/compose/compose-file/compose-file-v2/#command): The command for the container to run once it's built. In this case, we're running the command to initiate the server if we had installed the Triton Inference Server Library locally as [described here](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/quickstart.html#run-triton-inference-server).\n",
    "* [image](https://docs.docker.com/compose/compose-file/compose-file-v2/#image): The base image that we're building off of, in this case, the [Triton Inference Server](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags) image.\n",
    "* [shm-size](https://docs.docker.com/compose/compose-file/compose-file-v2/#shm_size): The amount of memory to share with the container. In this case, we're giving it 1 gigabyte for faster computation.\n",
    "* [ulimits](https://docs.docker.com/compose/compose-file/compose-file-v2/#ulimits): The max number of open file descriptors per process explained in this [Stack Overflow](https://stackoverflow.com/questions/24955883/what-is-the-max-opened-files-limitation-on-linux) post.\n",
    "* [ports](https://docs.docker.com/compose/compose-file/compose-file-v2/#ports): The ports to expose from the container.\n",
    "* [volumes](https://docs.docker.com/compose/compose-file/compose-file-v2/#volume-configuration-reference): A directory that can be shared between a container and it's host.\n",
    "\n",
    "Below is the a Docker compose file very similar to what we used to set up this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; margin: 0; }\n",
       "td.linenos pre { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos pre.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #408080; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #BC7A00 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #FF0000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #00A000 } /* Generic.Inserted */\n",
       ".output_html .go { color: #888888 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #7D9029 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #A0A000 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #BB6688 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"nt\">version</span><span class=\"p\">:</span> <span class=\"s\">&quot;2.3&quot;</span>\n",
       "\n",
       "<span class=\"nt\">volumes</span><span class=\"p\">:</span>\n",
       "  <span class=\"c1\"># Allows us to write files to Triton from JupyterLab</span>\n",
       "  <span class=\"nt\">model_repository</span><span class=\"p\">:</span>\n",
       "\n",
       "<span class=\"nt\">services</span><span class=\"p\">:</span>\n",
       "  <span class=\"c1\">#Each service is a Docker container. Below is how to pull Triton from NGC.</span>\n",
       "  <span class=\"nt\">triton</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">command</span><span class=\"p\">:</span> <span class=\"s\">&quot;tritonserver</span><span class=\"nv\"> </span><span class=\"s\">--model-repository=/models</span><span class=\"nv\"> </span><span class=\"s\">--exit-on-error=false</span><span class=\"nv\"> </span><span class=\"s\">--model-control-mode=poll</span><span class=\"nv\"> </span><span class=\"s\">--repository-poll-secs=30&quot;</span>\n",
       "    <span class=\"nt\">image</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">nvcr.io/nvidia/tritonserver:20.12-py3</span>\n",
       "    <span class=\"nt\">runtime</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">nvidia</span>\n",
       "    <span class=\"nt\">ipc</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">host</span>\n",
       "    <span class=\"nt\">shm_size</span><span class=\"p\">:</span> <span class=\"s\">&quot;1g&quot;</span>\n",
       "    <span class=\"nt\">ulimits</span><span class=\"p\">:</span>\n",
       "      <span class=\"nt\">memlock</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">-1</span>\n",
       "      <span class=\"nt\">stack</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">67108864</span>\n",
       "    <span class=\"nt\">ports</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">8000:8000</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">8001:8001</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">8002:8002</span>\n",
       "    <span class=\"nt\">volumes</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">model_repository:/models</span>\n",
       "\n",
       "  <span class=\"c1\"># This is the JupyterLab environment. We build it with a Dockerfile.</span>\n",
       "  <span class=\"nt\">lab</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">build</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">.</span>\n",
       "    <span class=\"nt\">runtime</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">nvidia</span>\n",
       "    <span class=\"nt\">environment</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">JUPYTER_TOKEN</span>\n",
       "\n",
       "    <span class=\"nt\">volumes</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">./task/:/dli/task/</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">model_repository:/dli/task/models</span>\n",
       "\n",
       "  <span class=\"c1\"># Prometheus</span>\n",
       "  <span class=\"nt\">prometheus</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">image</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">docker.io/prom/prometheus:latest</span>\n",
       "    <span class=\"nt\">volumes</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">./prometheus.yml:/prometheus.yml</span>\n",
       "    <span class=\"nt\">command</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"s\">&quot;--config.file=/prometheus.yml&quot;</span>\n",
       "    <span class=\"nt\">ports</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">9090:9090</span>\n",
       "\n",
       "  <span class=\"c1\"># Nginx is how we make the container available to sudents over the internet.</span>\n",
       "  <span class=\"nt\">nginx</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">image</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">nginx:1.15.12-alpine</span>\n",
       "    <span class=\"nt\">volumes</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"l l-Scalar l-Scalar-Plain\">./nginx.conf:/etc/nginx/nginx.conf</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{n+nt}{version}\\PY{p}{:} \\PY{l+s}{\\PYZdq{}}\\PY{l+s}{2.3}\\PY{l+s}{\\PYZdq{}}\n",
       "\n",
       "\\PY{n+nt}{volumes}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} Allows us to write files to Triton from JupyterLab}\n",
       "  \\PY{n+nt}{model\\PYZus{}repository}\\PY{p}{:}\n",
       "\n",
       "\\PY{n+nt}{services}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{}Each service is a Docker container. Below is how to pull Triton from NGC.}\n",
       "  \\PY{n+nt}{triton}\\PY{p}{:}\n",
       "    \\PY{n+nt}{command}\\PY{p}{:} \\PY{l+s}{\\PYZdq{}}\\PY{l+s}{tritonserver}\\PY{n+nv}{ }\\PY{l+s}{\\PYZhy{}\\PYZhy{}model\\PYZhy{}repository=/models}\\PY{n+nv}{ }\\PY{l+s}{\\PYZhy{}\\PYZhy{}exit\\PYZhy{}on\\PYZhy{}error=false}\\PY{n+nv}{ }\\PY{l+s}{\\PYZhy{}\\PYZhy{}model\\PYZhy{}control\\PYZhy{}mode=poll}\\PY{n+nv}{ }\\PY{l+s}{\\PYZhy{}\\PYZhy{}repository\\PYZhy{}poll\\PYZhy{}secs=30}\\PY{l+s}{\\PYZdq{}}\n",
       "    \\PY{n+nt}{image}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{nvcr.io/nvidia/tritonserver:20.12\\PYZhy{}py3}\n",
       "    \\PY{n+nt}{runtime}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{nvidia}\n",
       "    \\PY{n+nt}{ipc}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{host}\n",
       "    \\PY{n+nt}{shm\\PYZus{}size}\\PY{p}{:} \\PY{l+s}{\\PYZdq{}}\\PY{l+s}{1g}\\PY{l+s}{\\PYZdq{}}\n",
       "    \\PY{n+nt}{ulimits}\\PY{p}{:}\n",
       "      \\PY{n+nt}{memlock}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{\\PYZhy{}1}\n",
       "      \\PY{n+nt}{stack}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{67108864}\n",
       "    \\PY{n+nt}{ports}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{8000:8000}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{8001:8001}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{8002:8002}\n",
       "    \\PY{n+nt}{volumes}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{model\\PYZus{}repository:/models}\n",
       "\n",
       "  \\PY{c+c1}{\\PYZsh{} This is the JupyterLab environment. We build it with a Dockerfile.}\n",
       "  \\PY{n+nt}{lab}\\PY{p}{:}\n",
       "    \\PY{n+nt}{build}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{.}\n",
       "    \\PY{n+nt}{runtime}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{nvidia}\n",
       "    \\PY{n+nt}{environment}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{JUPYTER\\PYZus{}TOKEN}\n",
       "\n",
       "    \\PY{n+nt}{volumes}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{./task/:/dli/task/}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{model\\PYZus{}repository:/dli/task/models}\n",
       "\n",
       "  \\PY{c+c1}{\\PYZsh{} Prometheus}\n",
       "  \\PY{n+nt}{prometheus}\\PY{p}{:}\n",
       "    \\PY{n+nt}{image}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{docker.io/prom/prometheus:latest}\n",
       "    \\PY{n+nt}{volumes}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{./prometheus.yml:/prometheus.yml}\n",
       "    \\PY{n+nt}{command}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+s}{\\PYZdq{}}\\PY{l+s}{\\PYZhy{}\\PYZhy{}config.file=/prometheus.yml}\\PY{l+s}{\\PYZdq{}}\n",
       "    \\PY{n+nt}{ports}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{9090:9090}\n",
       "\n",
       "  \\PY{c+c1}{\\PYZsh{} Nginx is how we make the container available to sudents over the internet.}\n",
       "  \\PY{n+nt}{nginx}\\PY{p}{:}\n",
       "    \\PY{n+nt}{image}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{nginx:1.15.12\\PYZhy{}alpine}\n",
       "    \\PY{n+nt}{volumes}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{l+lScalar+lScalarPlain}{./nginx.conf:/etc/nginx/nginx.conf}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "version: \"2.3\"\n",
       "\n",
       "volumes:\n",
       "  # Allows us to write files to Triton from JupyterLab\n",
       "  model_repository:\n",
       "\n",
       "services:\n",
       "  #Each service is a Docker container. Below is how to pull Triton from NGC.\n",
       "  triton:\n",
       "    command: \"tritonserver --model-repository=/models --exit-on-error=false --model-control-mode=poll --repository-poll-secs=30\"\n",
       "    image: nvcr.io/nvidia/tritonserver:20.12-py3\n",
       "    runtime: nvidia\n",
       "    ipc: host\n",
       "    shm_size: \"1g\"\n",
       "    ulimits:\n",
       "      memlock: -1\n",
       "      stack: 67108864\n",
       "    ports:\n",
       "      - 8000:8000\n",
       "      - 8001:8001\n",
       "      - 8002:8002\n",
       "    volumes:\n",
       "      - model_repository:/models\n",
       "\n",
       "  # This is the JupyterLab environment. We build it with a Dockerfile.\n",
       "  lab:\n",
       "    build: .\n",
       "    runtime: nvidia\n",
       "    environment:\n",
       "      - JUPYTER_TOKEN\n",
       "\n",
       "    volumes:\n",
       "      - ./task/:/dli/task/\n",
       "      - model_repository:/dli/task/models\n",
       "\n",
       "  # Prometheus\n",
       "  prometheus:\n",
       "    image: docker.io/prom/prometheus:latest\n",
       "    volumes:\n",
       "      - ./prometheus.yml:/prometheus.yml\n",
       "    command:\n",
       "      - \"--config.file=/prometheus.yml\"\n",
       "    ports:\n",
       "      - 9090:9090\n",
       "\n",
       "  # Nginx is how we make the container available to sudents over the internet.\n",
       "  nginx:\n",
       "    image: nginx:1.15.12-alpine\n",
       "    volumes:\n",
       "      - ./nginx.conf:/etc/nginx/nginx.conf"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.display.Code(filename=\"assets/docker-compose.yml\", language=\"yaml\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's focus on the `prometheus` service. It's simpler than the `triton` service, but we still need to provide it with a [configuration file](https://prometheus.io/docs/prometheus/latest/configuration/configuration/). Here are some of the keys.\n",
    "\n",
    "* `global`: Defines properties we want to add to every  Prometheus job.\n",
    "  * `scrape_interval`: How often a job pulls from a data source\n",
    "  * `external_labels`: The labels to add to any tine series or alert\n",
    "* [scrape_configs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config): Specifies a target and how to interact with it.\n",
    "\n",
    "In this case, we're telling it to pull information from `triton` (as defined in the Docker Compose file above) metrics every 5 seconds. We can give this process a `job_name` to make it easier to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; margin: 0; }\n",
       "td.linenos pre { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos pre.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #408080; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #408080; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #408080; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #BC7A00 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #408080; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #408080; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #FF0000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #00A000 } /* Generic.Inserted */\n",
       ".output_html .go { color: #888888 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #7D9029 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #999999; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #D2413A; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #A0A000 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #BB6688 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"nt\">global</span><span class=\"p\">:</span>\n",
       "  <span class=\"nt\">scrape_interval</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">15s</span>\n",
       "  <span class=\"nt\">external_labels</span><span class=\"p\">:</span>\n",
       "    <span class=\"nt\">monitor</span><span class=\"p\">:</span> <span class=\"s\">&#39;codelab-monitor&#39;</span>\n",
       "\n",
       "<span class=\"nt\">scrape_configs</span><span class=\"p\">:</span>\n",
       "  <span class=\"p p-Indicator\">-</span> <span class=\"nt\">job_name</span><span class=\"p\">:</span> <span class=\"s\">&#39;prometheus&#39;</span>\n",
       "    <span class=\"nt\">scrape_interval</span><span class=\"p\">:</span> <span class=\"l l-Scalar l-Scalar-Plain\">5s</span>\n",
       "\n",
       "    <span class=\"nt\">static_configs</span><span class=\"p\">:</span>\n",
       "      <span class=\"p p-Indicator\">-</span> <span class=\"nt\">targets</span><span class=\"p\">:</span> <span class=\"p p-Indicator\">[</span><span class=\"s\">&#39;triton:8002&#39;</span><span class=\"p p-Indicator\">]</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{n+nt}{global}\\PY{p}{:}\n",
       "  \\PY{n+nt}{scrape\\PYZus{}interval}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{15s}\n",
       "  \\PY{n+nt}{external\\PYZus{}labels}\\PY{p}{:}\n",
       "    \\PY{n+nt}{monitor}\\PY{p}{:} \\PY{l+s}{\\PYZsq{}}\\PY{l+s}{codelab\\PYZhy{}monitor}\\PY{l+s}{\\PYZsq{}}\n",
       "\n",
       "\\PY{n+nt}{scrape\\PYZus{}configs}\\PY{p}{:}\n",
       "  \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{job\\PYZus{}name}\\PY{p}{:} \\PY{l+s}{\\PYZsq{}}\\PY{l+s}{prometheus}\\PY{l+s}{\\PYZsq{}}\n",
       "    \\PY{n+nt}{scrape\\PYZus{}interval}\\PY{p}{:} \\PY{l+lScalar+lScalarPlain}{5s}\n",
       "\n",
       "    \\PY{n+nt}{static\\PYZus{}configs}\\PY{p}{:}\n",
       "      \\PY{p+pIndicator}{\\PYZhy{}} \\PY{n+nt}{targets}\\PY{p}{:} \\PY{p+pIndicator}{[}\\PY{l+s}{\\PYZsq{}}\\PY{l+s}{triton:8002}\\PY{l+s}{\\PYZsq{}}\\PY{p+pIndicator}{]}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "global:\n",
       "  scrape_interval: 15s\n",
       "  external_labels:\n",
       "    monitor: 'codelab-monitor'\n",
       "\n",
       "scrape_configs:\n",
       "  - job_name: 'prometheus'\n",
       "    scrape_interval: 5s\n",
       "\n",
       "    static_configs:\n",
       "      - targets: ['triton:8002']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Code(filename=\"assets/prometheus.yml\", language=\"yaml\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional resources on how to utilize the Triton Inference Server's metrics, please consult:\n",
    "\n",
    "* [Triton Inference Server Metrics Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/metrics.md)\n",
    "* [Saleforce Blogpost on Benchmarking Triton Inference Server](https://blog.einstein.ai/benchmarking-tensorrt-inference-server/)\n",
    "\n",
    "\n",
    "<a id=\"next-steps\"></a>\n",
    "### Next Steps\n",
    "\n",
    "As this online environment already has a Triton Inference Server deployed, our last challenge to you is setup Triton on your own hardware. We hope the above resources can provide a starting point. For more information, please check out the [Getting Started Guide](https://github.com/triton-inference-server/server/blob/r21.11/docs/quickstart.md).\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"><img src=\"./assets/DLI_Header.png\"></a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
